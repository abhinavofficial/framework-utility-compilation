https://www.youtube.com/watch?v=dmL0N3qfSc8

https://www.youtube.com/watch?v=7ooZ4S7Ay6Y

https://github.com/AlpineNow/SparkAutoTuning

https://github.com/qubole/sparklens/

https://github.com/qubole/uchit/

https://github.com/umayrh/sparktuner/

## Internal working of spark

It is based on Spark 0.9.1. It does layout foundational thought process - pretty much the same as [Amp-Lab discussion](Feb-2022-AmpLab-spark.md) 

> Source: https://www.alibabacloud.com/forum/read-471

Job generation and running
A simply procedure of generating a job is as follows:
1. First, the application creates the SparkContext instance, for example, Instance sc.
2. Then, the application generates the RDD using the SparkContext instance.
3. Through a series of transformation operations, the original RDD is transformed to an RDD of another type.
4. When an action operation acts on a transformed RDD, it will call the runJob method of the SparkContext instance.
5. The sc.runJob call is the starting point of a chain of following actions. A crucial change happens in this process.
 The calling path is outlined below.
1. sc.runJob > dagScheduler.runJob > submitJob
2. DAGScheduler::submitJob will create the JobSummitted event and send the event to the nested eventProcessActor class.
3. After receiving the JobSubmmitted event, the eventProcessActor calls the processEvent function.
4. The job undergoes the stage transition and generates the finalStage and is then submitted for running. The key is the call of submitStage.
5. In the submitStage, the dependency between stages will be calculated. The dependency relationship consists of the wide dependency and narrow dependency.
6. During the calculation, if the current stage is found to have no dependency or all of its dependencies have been ready, the task is submitted.
7. The task submission is completed by calling the submitMissingTasks function.
8. The TaskScheduler manages the workers on which the task is truly run, that is, the above submitMissingTasks will call TaskScheduler::submitTasks.
9. In TaskSchedulerImpl, the backend will be created based on the current running model of Spark. If Spark runs in the Standalone model, the LocalBackend is created.
10. LocalBackend receives the ReceiveOffers event passed in by the TaskSchedulerImpl.
11. receiveOffers > executor.launchTask > TaskRunner.run
Code piece executor.lauchTask

```
def launchTask(context: ExecutorBackend, taskId: Long, serializedTask: ByteBuffer) {
    val tr = new TaskRunner(context, taskId, serializedTask)
    runningTasks.put(taskId, tr)
    threadPool.execute(tr)
  }
```

With so much said, the kernel is that the final logical processing occurs in the executor of the TaskRunner.
The calculation results are packaged into MapStatus and fed back to DAGScheduler through a series of internal message passing. This message passing path is not very complicated and you can outline it if you are interested. 