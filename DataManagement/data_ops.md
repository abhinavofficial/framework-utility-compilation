# Data Ops

## Introduction to DataOps

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>So as an introduction, I think the perspective of DataOps is encapsulated in the sense that what you do is much less important than how you do it. This comes first from manufacturing, and that when you're building something, you're building the factory...Building the machine that makes the machine is actually really important. And I think that perspective comes through with DataOps.&nbsp;</p><p>And then for people who work on the factory: you know, in a factory, there's often problems. Dr. Deming, who studied this, found that 94% of the time, it's not a person failing, it's the system that the person works. And it's the factory process, itself, that's to blame. Those two perspectives really set out the way that we think in DataOps. In data analytics, we spend a lot of time on the model or the algorithm or the data feature, or how we transform data, how we visualize data, how we govern data, even the data itself. So there's a very opposite view here that how you do these works, not what you do, not what model or what data or what visualization--how you develop it, how you deploy it, how you monitor it, how fast you iterate, how you collaborate upon, and how you measure is actually much more important than the things that you do. And it's a very contrarian perspective, because a lot of people talk about tools and technology and data. And that's all we talk about.&nbsp;</p><p>I think a DataOps is much more interested in people in the process, the pipelines that make all that process possible. And you're focused on very much giving the customer value.&nbsp;</p><p>So what do we mean by that? Well, I think it has to do with some idea of focus and mindset. So the first focus is, how do you lower error rates? How do you actually stop having problems in production and worrying that when you walk into the office on Monday morning, you're going to get a Slack message that something's wrong? Another mindset is, is how do you decrease the cycle time of change? And how do you continuously change and do it in a way that doesn't cause a lot of problems? How can you go from the mind of your data scientist or data engineer or person doing data visualization into the hands of your customer, and then iterate and improve quickly? Another mindset is, instead of focusing on an individual, how do you make the whole team work together? How do you have people who work for the same boss or sometimes people who work for different bosses work together? And how do you have fewer meetings? How can you empower people to change with safety and control? And then finally, how do you measure your process? How do you get analytic about the processes that you use in analytics?</p><p>Unfortunately, a lot of teams suffer from these poor processes, and their cycle time is low. Think of it--it takes months to deploy, they are constantly going into the office and getting nasty grams from their customers saying the data is wrong, or the data looks weird, or their teams are fighting against each other. And that just makes for a frustrating world.&nbsp;</p><p>So as we've talked about and, hopefully, as evidence why you're here, we think that DataOps is the solution to that suffering. It really focuses on a set of technical practices, cultural norms, and architecture patterns, that does focus on those things. How do you have rapid cycles of innovation? How do you have low error rates? How do you collaborate across all these tools and people and technology and environments? And how do you clearly measure and monitor the results of that process? If you take these ideas together--Agile, which is a way of organizing software teams (the Agile Manifesto was written in 2001), and then DevOps, which is sort of a technical way to do software development, and Lean Manufacturing--you apply those ideas or principles to the more complicated world of data and analytics, you get DataOps. And it gives you a set of ideas and methodologies to empower your team to sort of retake the initiative. It's more about taking back control and empowering your team, given that we all sort of live in this, this tough environment with all these forces against us.&nbsp;</p><p>So how does the idea of DataOps fix this problem? First of all, the term DataOps has been around for a few years, but in just the past three or four years it has started to gain traction. I think Gartner put it on its hype cycle starting in July of 2019. Look at Google trends over the past two years as a proxy of market growth, and you can kind of see there's a 500% growth in search volume over time. So that's one indication that it's good. There's a lot more media articles, a lot of companies branding themselves as DataOps, my company got to be a cool vendor, and there's just a lot more buzz around the term DataOps. As a result, there's also just a lot more confusion about what it is and why you should do it. Like most tech terms, they sort of inflate--unfortunately, I think due to DataOps' popularity it probably will get to that point. However, we have striven to have a very clear definition of why do DataOps and what DataOps is.</p><p><br class="ProseMirror-trailingBreak"></p></div></div>

## Data Journey First DataOps

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>One of the interesting things is about six or seven years ago, we wrote the DataOps Manifesto. And what's interesting is kind of showing you the term DataOps has actually gone up. And this Manifesto, when we wrote it several years ago, we kind of said, “We're talking about DataOps,” but no one quite got it. And now? I don't know, 10, 20 thousand people have actually signed this.</p><p>From my standpoint, sort of reducing errors and defects, it's a very lean idea and the production of data insight—charts and models and data sets and integrated data sets and visualizations—is the key to success. So errors and defects are a killer and that's a very Lean Manufacturing idea. And from my experience, and our customer’s experience, of being shamed and blamed with problems that you didn't start, problems with the data being trapped by having sort of existing data processes that you don't understand and then sometimes fail. And the morning dread sort of sitting (and I experienced this in 2006 and 7) sitting in my car every morning not wanting to look at my Blackberry waiting for something to go wrong with, you know, the thousands of people who were using our data. I think we've talked a lot here about the stress and wasted productivity from sort of chasing errors and not having things perfect and also having your customers find problems.</p><p>So this manifesto is really about a method to watch over our data's complicated paths, kind of to avoid problems and errors and customer frustration and also, you know, increase your productivity and increase your happiness. And so, I guess from my perspective, the idea of a Data Journey is… It’s fundamental. And so we need some way to think about it and I think the first big idea is the difference between what should be and what is.</p><p>So when we build a data and analytic system, you've got a database and tools to transform data and languages and visualization tools. And the difference between what is—is it running? Is it late? Is the data right?—and what should be is often missed in an organization. And so, you should know what should be.</p><p>And then you should not hope that it's perfect. Hope is not really a strategy, hoping things work is kind of a recipe for failure. And honestly? It's surprising a large majority of people run their data and analytic systems on hope, or it's converse, they sort of wait for your customers to find problems, and that's just not OK. It's not OK to deliver something to your customer and have them call up and say, “It looks weird,” and you go, “Oh yeah, it is… and I'll, I'll fix it quick or I'll fix it in a month.”</p><p>And I'm gonna talk a bit about this, but we've got really complicated data architectures. You look at any data architecture, mention dozens of little boxes everywhere with servers and software and tools and links and, you know, maybe this is pejorative, but it's kind of built on almost performative complexity: How many boxes you have are almost your cooler. And so when you have a lot of little boxes that are all working together, you need a Mission Control, need to see if things are working through those boxes.</p><p>And as all of us who've done data and analytics: Don't trust your data providers. They're gonna, you know, whether they mean, whether they wanna do it or not, whether they know they're doing it or not, they're gonna give you bad data and kind of get used to it and protect against it.</p><p>And don't assume what worked last week will work today. You know, if everything ran great last week, well, the data could have changed, some code, could have changed, some server could have had a problem. And then when you find problems, hopefully, before your customers do find the exact source in those little boxes, find the instance of that little box and what workbook, what model, what job is running it.</p><p>And I think a lot of people focus on sort of perfect data quality is the raw data that we're getting, and that's really important, but it's not a cure-all. Even with perfect data quality, you still could have problems, you could still could have errors because the data is being joined and put together and visualized, et cetera. And a lot of us try to test, which is check to see if things are right in a manual way. I'll, I'll eye it up and see if it's right. And sort of the idea here is to avoid manual testing of anything data server CPU is the report showing the right numbers, just avoid it like the plague because it's, it's cause it, it, it's sort of akin to hope. If it sort of works in development it's guaranteed to break in production.</p><p>And we've talked a lot about this in our books on DataOps, but really you're running a factory. And the lessons of Toyota and Lean and Deming: that every tool along the way, the ingestion tool, the transformation tool, the database, the model, the visualization data prep tools, data governance tools are kind of workstations on that assembly line. And so you don't want to buy a car that is produced on a crappy assembly line that has a lot of errors, nor do you want insight produced on a, on a crappy assembly line.</p><p>And you know, we've written a lot about the more general concept and wrote a Manifesto years ago that I talked about, so, I think, we need a new idea and that’s called a Data Journey.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>The idea here is that you need a layer kind of on top of what already is running. You need an expectation layer. It tells you of all the myriad paths that data takes from where it gets into your organization to where it goes out to your customer. Is it right? Can I trust it? Is it on time? Does it make sense?</p><p>And it's a layer that really observes things and doesn't run things, run anything. Like we've got lots and lots of orchestration tools and so really, you've got to observe kind of across those tools and down the stack that they run. And so that across and down problem for every instance is hard. And when things go wrong in them, you want to know before your customers. So getting real-time status and then what's missing—here is the production context. And this often sits with sort of a person on your Operations Team. Maybe they've got a spreadsheet or a Word document that talks about, “OK, this is run and after this hasn't run… and…,” but there's no sort of context of what's running.</p><p>And that context also requires that there's a lot of components, right? Your servers and databases and jobs. How do they all fit together?</p><p>So, Ronald Reagan said of the Soviet Union, “Trust but verify,” and I think that's right. So trust comes from monitoring and checking the data in every tool on your data journey. And test and look for anomalies at every step.</p><p>And then finally, this idea of your data journey and having a place to put it actually makes it really useful, right? One is that you can share it, right? Everyone wants to know the production schedule: between the Engineers who built it, the people who use it, and all the Managers who get yelled at. What's running? What will be running? Is really important and then keeping track of what happened in the past sort of learning from errors trying to look at over the last two weeks. Have we met our SLAs? Have we had any serious data errors? Where was the problem? What can we do to fix it?</p><p>This idea of root cause analysis and being able to go in and look and learn? I think these are all great ideas. And so what, why would you want to do it? Well, I think there's two parts. One is that of course, it reduces errors and by reducing errors, it drives team productivity. That's the bottom bullet. The second is that it also can actually by having a data journey that's sort of tested. You can use it in development.</p><p>There's also the idea of a data journey to help regression test or do impact analysis is a good way to think about how data journeys exist from a similar concept called data lineage, which is a way sort of static analysis of your data system but doesn't tell you if things are late or if the data is wrong. But nobody ships software just with static analysis, just like no one should ship data and analytics. Just looking at lineage, you actually need, to run the system and test it to see if it's right.</p><p>And so what are the principles behind a journey? So data journeys are sort of across the little boxes, right? Where you take raw data, it's batch or streamed, you place it in the database and you ETL it or ELT it, you transform it or clean it, and you use it right in visualization or data science or sent it to somewhere else. And the characteristics are: there's multiple tools, multiple data sets, multiple paths through those data sets, multiple jobs, multiple architectures, multiple customers, and multiple people. So that's a lot of multiples everywhere in addition to boxes, right?</p><p>And then the other little thing that's a problem about this: the boxes are deep. So the problem could be in all the way to the bottom could be and you got something in the lower right here off your FTP run. Or that could be perfect and maybe your server is, is spinning out of control or maybe the thing that's running at your orchestrator fell over. And this is great if you've got some tasks, maybe your tests are returning errors. You have this sort of a cross-and-down problem in finding where things are. And that, that's the challenge here because things will always go wrong.</p><p>And so, you have lots of them in your organization already, right? Maybe you call them jobs or pipelines. Maybe you call them workflow and it, you know, they could be sort of embedded in different teams and their batch and streaming and manual. And they're everywhere. And if you look at larger organizations, they could have hundreds, even a team of, you know, 3 to 5 people, they could have 20 to 50 of these things running. And the whole point here is you just want to know that they're running and know that they're right and think about other things.</p><p>6And you know, we all have lots of tools as I've talked about before. And so people have different toolchains, lots of boxes, there's lots of tools in every category. And then the second is there's a lot of pieces that things run on. It's the across and down problem and the data journeys—these pipelines—actually fit in, in an organizational context, right? So you may have a Central Hub Team who ingest data, they've got their own journeys and then you've got each team in a, in a sort of line of business running separately. This could be mapped into a data mash context or each are domains in the inter-domain relationships. But, like, there's lots of hub and spoke dependencies here.</p><p>And so data journeys themselves have a development process, right? You change them. You wanna get new code or new data or new changes to configuration into production and oftentimes that development process is different depending upon your organization. And so, you know, because we've got these organization designs, have bespoke data match, your data journeys end up following Conway's law, in that they all have to be. You know, people own the data journeys based on the organizational structure, not based on, the perhaps, the best technical way to do it. And this happens in software, this happens in manufacturing. Conway's law is a very common thing. And, so we talked about this, right?</p><p>They need correlation. So if something goes wrong, right? You wanna know it, was it the raw data? Was it something happened when I put the raw data together with other data or the visualization? Is it making sense or it's just late and the server is getting pinned and then what will happen what's related to this that will break it. And so this idea of correlation across and down and between journeys is actually really fundamental because of this complexity and coupling in these systems.</p><p>And so a very common case if you look at the, how does one data journey relate to another? Well, one is causal, it completes and then something an orchestrator or a signal causes something else to happen, right? That's in the upper left here. Sometimes it's temporal like your ETL process has got to finish by 5 a.m. and then at 7 a.m., your dashboard build works and they're, they're uncoupled, but they're obviously related to each other. And then there's manual cases, you know, your, your Airflow schedule or finishes at 1 a.m., and then your India teams got to press a button to do something at 3 a.m. And then there's ones that are more event-driven; so something new, data drops, added in updated dashboards. And then these relationships are complicated between journeys. They sort of fan in where I've got a bunch of new data sets and I'm building the warehouse or I built the warehouse fan out and I've got a bunch of dashboards and data journeys that are related.</p><p>And so this tribal knowledge, unfortunately, really isn't, you know, it's not anywhere in the organization and that's why it's crazy hard when things go wrong and you're having to take all your smart people and, and putting them together in a room and that they're wasting their time trying to find out what the problem is—rather than having a journey just tell you.</p><p>And so, you know, the cornerstone of this is that you need an expectation layer on what is happening with the production of analytics and expectations could be on logs, could be on errors, could be on metrics, they could be on data tests. And so you need to judge what is versus what should be.</p><p>And so journeys react to events and notify. What's happening in your system, the sort of living breathing data, moving from place to place, something happening with data, results being pushed to customers. This is good. Sort of living and breathing and going on. And so what's the variance between what should be and what's not. Like obviously, schedules and durations and dependencies, “This should have run before the other thing.” And then trust. And so that variation between what you think should happen and what is… is the source of a great way to notify.</p><p>And so, you know, because you've got this database of what happens, you can actually do some historic analysis and look at it and say, well, what did happen and what happened last week, what happened last month and where are we? And sort of looking at it from the Boss's context. How many tests did you have? How often were you late? And then this idea of context right? Between someone who does the work, right? Maybe I'm a Data Developer, a Data Scientist, or Engineer. Maybe someone who runs the work, a Production Engineer, maybe someone who manages people, and then maybe someone who uses it. Like they're all interested, “Is this working or not?”</p><p>And so this shared context is actually really important for people and oftentimes it's missed. And some people try to do it right, they'll have a run table and they'll manually create things and that, that's very helpful. But it's just, it saves a lot of time if you can give the pulse of what's going on with everyone else in the organization.</p><p>And so the idea is that, you know, we are missing a piece. That piece is the concept of a data journey. It's something that is your expectation layer on top of all your systems. It tracks as an instance, each one of these paths that data takes through your organization and allows you to store that data, get notified on that data, set expectations on that data.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>I'm gonna talk a little bit about five, the five pillars of the Data Journey. But the main problem is, you know, errors are in a lot of places, right? They're not just in your data. And you know, we're gonna talk about testing your data today a lot, but you know, data problems are frequent and often. And so you know, reducing errors along that journey is very important for data teams. And if you do it, it actually gives you time to do more things. And so that path from data source to customer value, we call a Data Journey. And I'm just gonna quickly go through these Five Pillars mainly as context for what we talk about.</p><p>So, so in this diagram, the red parts here pretty typical, it's a bunch of data sources. Maybe there's a load process in here. Maybe there's a transform process, maybe there's a predict process, maybe there's a report process. There's your data, your infrastructure. And kind of functionally, where these ideas go, is, we think, the Five Pillars. A Data Journey involves data at rest, but also talks about data in use, and also talks about looking across all the steps that data takes and looking down all those steps, sort of down the stack. Because problems aren't just in data, they could occasionally be in your server. And then setting expectations on all this. So you know that it's right before your customer sees it.</p><p>And so the first one is looking, the first pillar of a data journey is kind of looking across your steps and seeing if things are wrong in order and sometimes you have tools that are a collection, they're unordered or tables that are unordered. But a lot of times the data journeys have an order to them, right? You have to do your data work before you do your model work, before you do your business work and just making sure that that order happens, that SLAs are met, that timing's met that that there aren't delays and really getting the process lineage and the process reliability, right, is the first pillar.</p><p>And then the second pillar is, well, when things go wrong, they kind of go wrong in different places. A lot of times it's the data, but it could be, as I said, your server or your infrastructure or bad code or your network could be CPU it could be disk space and sort of monitoring those things in a very it way is also important to do it. And that's just, is your tech stack working? And then, is your data right? You know, is it schema right? Is it fresh, is it volume? Is it right from a business perspective, like have your sales gone up or gone down? Is the percentage that they've gone up or gone down? And that's really about what a lot of people call data quality.</p><p>For us, we have a little more expanded because it's not just about the raw data. It's also about about about how you put the data together. And so we're gonna talk about data at rest or the data quality validation, testing of data at rest. And you know, the other part is that you have data that's not in a database, but it's in used. It's actually in a predictive model or in a visualization. And how do you test that because those are software tools that are driven by code that could have errors.</p><p>And so how do you make sure the user experience is right with those tools? How do you make sure they trust the data even though the data in your database could be perfect. Somebody in a model or a visualization could have worked with it, it could be wrong and your customer could find it wrong.</p><p>And then lastly, sort of the expectation layer like you've got, you know, your data journey should run with a certain CPU your data, raw data, and integrated data should be perfect, your model should have this level of prediction, and how all these things have to happen in this order. And then how do you collect all those expectations and tell people and notify and that's really about alerting and sharing that information.</p><p>And so, you know, how do you test data at rest in a database? As an example, how do you do that? And so the real question goes, is sort of, “Why?” So we've, we've talked about sort of what's the problem with data and analytic teams. And I think a lot of times is that, that their projects fail or that when they get something in production, no one trusts the data, no one trusts the teams, they have too many errors. We did a survey recently where teams are just stressed and unhappy. So if you look at the data on what data and analytic teams are doing, it's, it's a very sort of unhappy state projects are failing. The people don't trust the data and teams are stressed out.</p><p>And so for us, the way that we've looked at the world is sort of opposite of a lot of people. We don't think the problem is that your database isn't too fast or your ETL tool isn't easy enough to use. We think that there's a system that you need to build around your data and analytics estate and the purpose of that system is to drive these three values sort of lowering errors in production, increasing the cycle time at which you can get new things into production with low risk.</p><p>And then just trying to automate various parts of your of, of the work that your, your data and analytic team does. And you know, I think there's a lot of reasons why, but I think just one of the reasons is and there is a lot of data tools out there. And so every day it's the sort of Cambrian explosion of data tools across everything: databases, data sources, ETL tools, visual visualization tools, governance tools, data science tools AI tools.</p><p>And so you know the way, the way we think about it at DataKitchen, is we, we want you to focus first on the Data Journey. So focus first on lowering the number of errors in production. And for ous, errors, we use the term “errors” importantly because we think it's not just data quality, it's not how good your raw data is that can be very important. But it's also all the other steps where it's raw data to integrated data, to being used in a tool. And all those things need to be put together.</p></div></div>

## DataOps Production Pipeline

<div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>One way to talk about DataOps is to say, "Well, what do you do in DataOps?" There are a lot of things you do, but let's talk about three pipelines.&nbsp;</p><p>In those three pipelines, the first one is really about how you orchestrate data to customer value. Analytic processes are like manufacturing. If you look at this value pipeline (I'm going to use the term value and production pipeline kind of interchangeably in this because really, they're about taking data on the left-hand side and manufacturing artifacts from it: charts, graphs, refined data sets, all the way out to the right-hand side to the customer)--it goes through these...think of them as manufacturing stations, where you could access it or transform or model and visualize. And think of the tools that are working on those manufacturing stations. There are data viz tools, or science or catalog or pipeline, there's tons of tools in each category. But the fundamental metaphor is that we are really manufacturing and whether it's streaming or batch, or big or small, you are running this factory of analytics.&nbsp;</p><p>And also these pipelines themselves, these abstractions, they're not owned by one thing, most of them are distributed. An IT team will own one, a self-service data team will own another part. And imagine an organization has hundreds of these pipelines that are being kind of virtually connected together. If a customer says there's something wrong, well, how do you know which pipeline caused it and which team caused it?&nbsp;</p><p>The second idea is that you have another pipeline, in addition to what's in production. It's how you get things into production, the innovation pipeline. And that's sort of like software development. How do you deploy, how you continuously integrate? When your team that's doing work whether they're developing in R or Python or Tableau or Looker, that the work that they create, or the configuration that they create is conceptually code in the same way software engineers are creating code.&nbsp;</p><p>And you've got this diverse set of tools and techniques and diverse people. And you've got to find some way to get that code from the person who created it into the hands of the customer. And that's the deployment process, the moving of code from a development person to production. And so we have to do both things: we've got to run this nice Toyota assembly line, and then we've got to pick things up from an assembly line and change it and put it back into production without breaking anything, and do it with low errors and fast cycle time and good collaboration and high measurement. And so those are two pipelines, the value pipeline in production, the innovation pipeline, which is deployments.&nbsp;</p><p>But we've actually got a third one that underlies them both, which is the environment pipeline. Pipelines are processes that run in things. And what I mean by environment is how you actually build environments that reflect Dev and Production. How do you build sandboxes and all the complexity out there into the hardware, software, data-security environment that they run in? And they're kind of the foundation of each one of these pipelines.</p></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>So let's look at this production or value pipeline. We're going to talk a lot about how they have a lot of errors, but also how we construct and monitor these pipelines, how they map into organizations, and, of course, how we can improve them.&nbsp;</p><p>Well, there's a lot of tools in data analytics and a lot of tools for individual contributors to do their work. Here's a market map of all of them, and it just keeps growing. They are open source and closed source, there's tools that are in the cloud or on-prem, you know, it's just a big marketplace, and people love their tools.&nbsp;</p><p>The second thing is that we did a survey with Eckerson, a great analyst company. We just asked this question, how many errors--incorrect data, broken reports, late delivery, customer complaints--do you have each month? Most companies have just way too many. I think that's a problem. If you're going to run a good Toyota production line, you don't produce cars that have defects. And so we as an industry are just producing a lot of defective analytics, which leads to a bunch of problems.&nbsp;</p><p>So that's where another metaphor, that we talk about in DataOps, comes in: it's really not about data quality, it's really about low rates of error. Because you could have poor data quality, which actually could lead to an error, or you could have perfect data quality and something happened in the processing. The user, your customer, doesn't know at the end. It's just wrong, or it's just late, or it's problematic, or it's not right. So really focusing on error rates, as opposed to data quality is another perspective in DataOps.&nbsp;</p><p>So you know, you've got a lot of tools, we've obviously got a lot of data, a lot of errors in production. But these teams who actually create things that go into these pipelines kind of work in an integrated value chain for their customers. Maybe it's someone in the IT data center providing data, some data engineers transforming data, data scientists modeling and putting algorithms on, people are doing data visualization and governance. And if you look at these people, this value chain, there are a lot of tools that go on. Here's an example of one where data comes in from a service or CSV files, it gets in some kind of lake, maybe there's some MDM applied, it does some transformation in Talend, or SSIS, does some data science, ends up in a database, actually ends up in being visualized, and gets in a wiki. But that value chain has an associated tool chain that goes with it.&nbsp;</p><p>If you look at what those tools do, like when you're getting data or accessing data, there are servers and storage and databases, and software and FTP. If I just want to put a new table and have a report on it for my customers, I've got to access it through IT, I've got to put it in a schema in a database in data engineering, perhaps in data science I've got to segment that table. I've got to visualize it, and get it to my customer. And then I've got to actually keep track of what those columns mean and where they came from. And I do that in a catalog. And just putting a simple table into making it visible to your customer, you've got a lot of tools, and perhaps a lot of teams that are involved in that. And so no wonder things are slow.&nbsp;</p><p>And here's a perspective that since everyone's got their own tool to do that, they're working independently in their tool, yet they're dependent on what someone else does. As a data engineer, I'm dependent upon IT to get that data somewhere. And if I'm a data scientist, I'm dependent upon someone in data engineering. And we each own our part and they have specific tasks that they're working on. And think of what every tool does. Every tool has some format of the file that it creates--it's a tableau workbook, it's a workflow. And think of those tools as having their own series of steps that have to happen their own workflow, their own DAG. (A DAG is a directed acyclic graph that says a graph of steps.) So every tool is doing some things in succession. And maybe it's just like a Jupyter Notebook, or it's one step after the other. Maybe it actually is a directed graph, like Airflow. And so because you've got a lot of tools, and you have a lot of workflows or DAGs, you need a meta-workflow, a workflow on top of the workflow to organize the production system.&nbsp;</p><p>So the production value pipeline is really a meta-workflow or a DAG of DAGs because we've got all these tools. And because each one of these tools has its sub-workflow. That's an interesting idea. But I think this is true because most organizations have two or three different ways that they want to do their data engineering, a couple of different biz tools, a couple of different data science platforms to make this work. So another DataOps perspective here is that there's not too much data in the world. It's that there are too many people and too slow processes to take advantage of that data.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>Given the situation with our production pipelines, let's talk about the benefits of automation or how can we improve them. So the first idea is think of your production as a manufacturing line. And this is the idea that we all work on this technically complicated thing together, that's an assembly line. If you focus on the processes that act on it, you can have increased product quality, you reduce the amount of rework that you do, you end up with higher employee satisfaction, and therefore higher profits in this sort of Lean Manufacturing idea.&nbsp;</p><p>Total Quality Management, and a bunch of different words, has really been transformative and has gone from something that just the Japanese did in the 1980s to being standard throughout the world. And so in production pipelines, what we want you to do is think of them like a factory. This diagram at top is illustrative. You've got your production data going in one side, it goes through the series of steps that we talked about the manufacturing line, and on the other side, you've got your analytic customers. Think of a big stoplight at the top. When this is running, you have a green light that says everything's working, and my customers are not going to find any problems. With the data that's going through the processing, I know that it's going to be right, and I know my customers are going to be happy, and maybe have a yellow light saying something's weird.&nbsp;</p><p>So think of these tubes as pipelines, and everyone's working in a pipeline. And there's lots of tools and lots of data going in and lots of customers on the other side. Companies have hundreds of these pipelines running all over the organization on a day in and day out basis. One of the most important things here is that you need that light at the top saying, hey, it's working or not. Because if you don't know, if you just hope it works, then you end up in the cycle of rework. And it's just like in a factory--if you're producing cars with defects, it's so much more expensive to fix the defect than rather fix it in the first place.&nbsp;</p><p>All these pipelines, themselves, they're all interacting with your tools and data. You've got your database tools, and science tools and catalog tools and ETL tools, databases, data lakes, and they're all trying to work with these tools. Because it's a DAG of DAGs, they're each doing their part of the work. If you want to have this stoplight on top that's going to tell you whether your customers are happy or not when all these pipelines are running, how do you actually do that?&nbsp;</p><p>Well, the first thing is you need to add automated tests and monitoring in production. So you need to test every step and every tool in your value pipeline. Because you know, hope is not a strategy. Don't trust your data suppliers, don't trust your servers to run. It's sort of trust-but-verify. And put on top of your entire process, a set of tasks or logic to see, "am I getting bad data? And is my business logic still correct based on that data? Are my outputs consistent for my users? Is there some big variation between what a business user saw current versus previously?" These sort of tasks are a way to get analytical about your production.&nbsp;</p><p>Here's an example of what in our product we call a recipe which is a DAG, a workflow, a series of steps. And here's an example of a customer that was trying to build a data warehouse. These numbers represent the numbers of tests, and you can see create dimensions, create facts, create source tables. But one of the ideas is to have tests that happen earlier in your production process. Let's say something happens in the creation of a dimension, well, if the dimension is empty, everything else doesn't make any sense.&nbsp;</p><p>You can do all your work, but why not stop what's production, learn about that sooner, and maybe you can correct the problem before it gets out and you're not late. And again, think of an assembly line. If you're going to put a wheel on a car and you notice that it's flat. Well don't put that wheel on the car, and maybe even stop the assembly line so you can get another wheel to go on it.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>When you think about trying to monitor what's happening, it's not just things like server and CPU monitoring. Those are important, but it really is getting into the data and the artifacts that are created from that data, and then classifying them as something's wrong. If you’ve got a file that you expect is going to always have a million rows and you got 10 rows in it, stop the assembly line, pull the virtual Andon Cord, raise a red alert, and get someone to look at it. Because that's going to affect everything else. And don't wait for your customers to find out the problem. Don't live with the idea that that's just acceptable.</p><p>Another feature in our software is that once you find these problems, you can actually alert and maybe create a JIRA ticket, send an email, send a Slack message. Everyone has an SLA; every customer—once they start getting a report or an analytic—gets used to it coming on a certain date and time. Sometimes it's more weeks than others that you've got to patchwork or run around. And the earlier you have an alert, the more time you have to patch it. Perhaps it's as simple as asking the data provider date to send the data again. Or maybe it's more complicated, and you actually have to write a patch on the data. Or it maybe it’s so complicated that you have to stop and wait for something to be fixed. But it's important that you know about it before your customer does.</p><p>There are a lot of tests that can happen on data. There are tests that look at the data itself. There’s whole class of data quality profiling tools that will profile the data and look for things like, “Does this column have three values in it?” and you can test to see if there are three values. There are business logic tests, and you can write those tests in lots of different ways. You can write them in SQL; you can write them in your favorite tool. And there are test engines out there. From our belief in our software, we think that you should be able to write tests using whatever tool you want. So if you're a SQL person or an ETL person, or you'd like to write MapReduce jobs, you should be able to write your tests there.</p><p>One example of a test that is one of my favorites is called the <strong>location balance</strong> test. I was talking with a customer who's working in Azure. They get data from internal systems, it gets into a Blob storage, it gets into three levels within their Snowflake infrastructure. And then it gets cached in some views in Power BI. So you've got basically the same data item in four or five places. So how do you know that something hasn't gone wrong in each one of those steps? So we have something called a location balance, where you can basically say there's a million rows and you're going to get the equivalent of a million rows out. What that means is you need something, number one, that goes across the whole system, you need something that can keep counts and variables and sub-datasets that you can do comparisons across.</p><p>Our tool provides that, and here's a way that we wrote one. Here's the case: it's a stop-on error if final table row count, not equal to the expected row count. You can see the logic here that the final table row count should be equal to the expected final table row count. This is a case where you just want to make sure that if you’re moving data from a blob store to a database table, or from one level in a database to another level, that the counts are the same. And so it's a very simple way to make sure that you don't have any losses on these transforms.</p><p>Another one of my favorite tests is another aspect of running a good production pipeline. You want to make sure that you produce items that your business customers know is right and business people—and consumers—they're heuristic, they don't know all the details in the data, but they do know certain things very well. Like they often have 80/20 rules. So 80% of the value comes from 20% of their products or product groups, or regions or customer types. Well, I've had the problem of putting data in front of customers and having them within three seconds say it's wrong. That's very deflating. Sometimes it's wrong, not because the source data is wrong, sometimes there's cases where there's just small files with hundreds of rows that affect it.</p><p>Here's one type of test that tries to capture that business heuristic. So if you look on the left, there's what's in production that’s live, and then on the right, what's in pre-production. So if you look at this table—and let's say it's a hypothetical table—there's SKUs and products, and there's a grouping of products. And there's volume on each, and the volume' is 575 for this group, and they're grouped up in these two groups. Now, let's go to the next day or the next week. I've still got these products in the sale SKUs, but notice the product grouping has changed. However, the total volume is still pretty much the same. It's 575 or it’s 587, so if you compare just the total volume, it's probably within the lines of what you're going to see every week. But look at comparing the grouping here. As a business person. I know that group one’s got about 225 group two’s got 350, but suddenly group one's got 358. What if that means that's your competitor's product, and suddenly your competitors product went up 150% in one week? That's how a business person would know it's wrong because they have memorized what their competitors do.</p><p>So one example of a heuristic test that you should run is just to compare these main items between what a customer has set. We need to be able to create tests that represent this.</p><p>Again, what happens if this is a problem, and you get it? Well, I think there's a philosophy here that you're always going to have problems. And instead of seeing it as a failure that you did something wrong, it’s better that your team sees it as an opportunity for improvement. You can get you and your team to say, “Okay, what did we learn? How can we implement a test or a change in the process to do it?” I think that's actually a really important thing, because the data world is always going to have failures, because data providers are always going to give things that are broke. You can help notice those before [they cause problems]. Or even if you don't, you can have a way to improve.</p><p>So let me give you just an example of what I mean in our software here for a few minutes. I've got a simple example. You know we're DataKitchen, so we've got this overuse of food metaphors. There's these four steps, they're called a recipe,. This is our workflow, or DAG, and this is a really simple one—most of these have hundreds of nodes, they're in a graph, but it kind of represents what the way we think of the world. In this one, it's not doing everything in analytics, it's just loading a database. So again, a super simple example. But even in the simple example, it's complicated. We've got some scripts that run in Windows, we've got some Python code, we've got SSIS, which is Microsoft's ETL tool, and SQL Server. We’ve got four different tools here running to do this work of loading things in the database.</p><p>When these run, we want to be able to monitor what's happening, not only did these run correctly, but we want to sort of dig in and find out. This number here at the end is a three that represents that there's three tests. So when the system runs, it's actually provides a good set of data. We keep track of all the processing that has happened and the counts and even the code that was acting upon it. That creates something called an order in our system and the set of order runs, we group together. That allows you to start looking at trends across what happens with your order runs. If I go to this one on the right, I can see some time on the bottom. This is the time it ran and volume. And one of those sensors is just a simple row count—all we're doing is counting, counting, counting. Then it suddenly drops and then suddenly goes up again. And hey, that may be fine for your data, or hey, that may mean that this is the day that you start getting that call from the CEO that his dashboard is wrong. Suddenly you're scrambling on Saturday to fix it.</p><p>You should be able to find this stuff out, before it goes to your CEO, and make sure that it's right. The whole point is notice these things and be able to have warnings or failures, or logs and have alerts that go off. Because if something's wrong, you want to know about it. This idea of statistical process control breaking bounds is a very Lean Manufacturing concept. Likewise, even looking at timings—seeing if something's taking longer. I can't tell you the number of times I've had people say, “Yeah, it ran in two seconds, that was unusual.” That means the data didn't show up. Well, that's probably a good way to alert and find out if something's wrong.</p><p>We have this recipe, and when I ran it it creates a simple order run. And it actually got an error. This is also part of our aphorisms that you want to love your errors and find out your errors before your customers. Here, the very last step didn't work. What does that mean that it didn't work? Well, if I look at my test results, this test—this count raw orders row failed. The system was running, maybe every tool gave a correct finishing code, but the data itself was wrong. So we're digging into the data or digging into the artifacts that created that data to be able to tell. When you find it's wrong, maybe it's not the test, maybe there's something that did go wrong with a system. So we bring all your logs together.</p><p>Just to finish out this demo: What does it mean to actually dig into the data and look at it? So there's lots of ways that people like to do their data work. I interacted with my data by writing SQL. This is the source; this one is just simple. It's a select count. And that was actually pushed into our test framework. Here's how that went: that came in; it was a stop-on-error test. We took it from that key, and then we wrote this very simple logic. It's a count that's greater than a hard number. This number doesn't have to be a hard number. It could be a variable, or it could come from the history of a variable.</p><p>So why does this all matter? You want your production pipelines to run automatically. You want them to tell you if something's wrong before your customer sees it because you want to lower your error rates and embarrassment. So first of all, test and monitor automatically on top of production. And even behind that, meta-orchestrate all your tools so they can run automatically. Then do it on top of your entire toolchain. We want to be able to send alerts and notifications and keep track of the history of this running system because that history is actually a good source of analytics. We talked about these different test types, but again, why do you want to do this? Focusing on low errors actually has this ability to give you more innovation, because, in some ways, it freezes out people’s fear. If people have less fear, they're more willing to try things. Then your customers will give you more trust because they'll stop having so many errors.</p><p>It leads to less stress and less embarrassment to your team. So it's counterintuitive. If you focus on errors, you actually end up getting more value for your customers.</p><p>And again, another aphorism: no shame, no blame, love your errors. This is actually a phrase I've used quite a bit, because in a lot of organizations, when errors happen, it's seen that you screwed up and something's wrong with you. I think one of the ideas of Deming is that we're all sort of touching the elephant and these big complicated things and assembly line, a big analytic production process. And no one can have the whole thing in their head. So when there are errors, errors are going to happen, and don't blame someone try to find the root cause and try to find a way to fix them.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><u>Transcription</u></p><p>A lot of analytic teams aren't very analytic about their production processes. Keep track of some things: What are your error rates in production? What part of the pipeline and which pipeline are they running on? Which data provider is giving you crappy data? Are you meeting your SLAs? What's your coverage in the tests? These are very important metrics. We're trying to help people be data-driven, and it's kind of hypocritical if we as data analytic teams aren't analytic about what we do.</p><p>This is a report that we called a tornado report. It shows the weeks and the different data providers. We built this in conjunction with JIRA. We’re saying, on one hand, what's the severity of the error? And where did it come from? And then the on the other, how many hours of time that took to fix it. This gives you leverage on your data provider saying, “Look, the last three weeks, we had a sev-one error because of your data, and you cost me three days of time of my team to fix your data problem.” I think that kind of quantitative discussion can help you provide leverage on your data providers.</p><p>Second, looking on a project basis, or even on a pipeline basis, you should try to look at over time. Here's the middle bar—look at how many errors you have in that pipeline, or how many tests you have in that pipeline. Try to look at is that pipeline on time? Very simple things that I think can help you look. If you look across all these, you can start to find some patterns. Another part of metrics is that by measuring you provide evidence to change the behavior of your individual team. You say, “Look, we've had these errors. Here it is in the report. Do you believe this or not? Let's try to focus on reducing these errors.” It becomes less of your opinion, and more of here's the facts. You can all rally around to fix these facts.</p><p>And here's another aphorism: “Do you know what ‘don't be a hero’ really means? Well, it doesn't mean be heroic. It says don't solve problems, figure out how to never create them in the first place.” Again, it’s a different philosophy. A lot of organizations in data analytics praise the individual contributor who works the weekend to fix the problem. And yeah, I think people are good, but figure out how not to have that problem in the first place. Talk to that manager and tell that manager, “What the hell are you doing? Why did that happen? Why is that person working? How are you going to make sure this doesn't happen?” Again, because the manager owns that problem.</p><p>So what are the technical characteristics of good production pipelines? Well, the first is this idea of meta-orchestration. You've got all your tools, and you love them. Plug in your existing tools into the pipeline. The second part is that these pipelines themselves need to be monitored, tested, and observed. There’s a term from DevOps called ‘observability’. You should automate those tests, you should keep track of the test data and artifacts, you should send alerts, you should do things across the entire pipeline: statistical process control, end-to-end, and other test types. And then store the pipeline definitions, themselves, as code and keep your tool code and pipeline code together. Once each one of these pipelines run, keep a database of the run history of the operational metrics, because you can use that to improve. So that's the technical characteristics.</p><p>Let's look at the organizational characteristics of good production pipelines? Well, first, everyone's got a development team—Data Scientist, Data Engineer, and a Production or Operations Team. Having a good relationship with the team and being able to share common artifacts like the pipeline, the run results, everyone can look at it. We all can see what the problems are, we all can fix them quickly. Because your production team always hates errors, they want things that run automatically, easy, and with low errors. That relationship—don't see it as a data scientist or a data engineer just throwing it over the wall to your production operations team—we need to have a set of shared objects and a shared relationship. Part of that is a reflection and improvement mindset.</p><p>I'm going to talk a little bit about something called a quality circle that you can do right away and be able to have fast reaction to the inevitable issues. I'm also going to talk about something called Mission Control.</p><p>There's another idea called safety culture. That means if there is a problem, you should feel safe to bring it up and not be shamed or blamed. Finally, I'm going to talk a little bit about trust in team members rather short of deference to vendor control.</p><p>So first of all, this relationship between developers and production: they're a team and they need to work together. One way to do that is to have a quality circle in production. Our software can help you do that. But you can also just do this with a spreadsheet. Keep track of every error that you had in production, put it in a spreadsheet. Then on a periodic basis, meet with a team to review those errors and sort of find patterns and root causes. As part of that meeting, just pick one or two things and create a new task or a new procedure to mitigate those problems. Focus on error reduction and also focus on your team's psychological safety, that if they bring up a problem that they're not going to get stepped on or blamed. Have both Dev and Ops teams as part of that meeting, and have everyone try to look at it, because we all have different perspectives on it. Maybe you do this every three weeks, that's a half hour meeting and try to assign someone to fix that. You'll see actually, gradually over time that by focusing on the psychological safety by taking those errors, and just making them visible on a spreadsheet, you'll actually start to have it. This is just a super Japanese management technique that you can do with zero cost. You can do it today, and it's all focused on lowering errors in production.</p><p>I was talking with a vendor and they want to have the uber-tool that does everything. But the promise of a single package offering that can do everything in data and analytics is years away. A lot of vendors don't trust you to have the freedom and the choice to use your own tools. So I guess beware of the unified platform or beware the tool vendor that's selling you a promise that everything happens.</p><p>Another idea is set up a Mission Control. This diagram’s got a lot of pipelines running. You see all the pipelines running in different parts of your organization. You should monitor those. Maybe it's your production organization who's monitoring, maybe it's you, maybe it's your team—but try to look at what's happening. I think actually making that visible: having reports, having views, is an important way to rally whether things are working or not. And again, our view is it's not that they're just running, it’s that the data and the artifacts that are creating on them are correct so that your business customer knows that that's going to be right. And so that the stoplight is has a little bit more meaning on it.</p></div></div>

## DataOps Development Pipeline

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>So let's actually get to the innovation pipeline. What is this idea of innovation pipeline, and why is it a problem?</p><p>Let me start off with a very concrete case,. Let's look at a development lifecycle. You can see there are columns here. The first column on the left is an individual development environment. The second is a team development environment where there are more people. There's a third environment, which is your test and UAT environment. And then your fourth is your production. We've got to make sure that we can move things from one to the other.</p><p>And so who works in these environments? Well, a data scientist may work in their development environment. But as a team, they're working with a data engineer who is doing the data work to feed their model. And then there's perhaps a UAT team or a pre-production team that they've got to work with. And finally, there's a production team who runs it. So you've got these different groups of people collaborating across these environments.</p><p>And their development tools are different. Like for instance, a data scientist may use a Jupyter Notebook, but the data engineer may prefer SQL and Airflow or something. Likewise, the code that they work on is different, and especially the code version. That Jupyter notebook, that the data scientist is developing in their development environment, is going to be put together in the team development environment with the SQL that the data engineer is working on. And sometimes the versions get out of skew. And then the way that people run the actual hardware they work on, the operating systems and libraries, and even the test data are different across these environments. So how you move things from one to the other can be problematic.</p><p>Do you know for instance, that the Python library you've used on your desktop is the same that's on the team environment, is the same that’s in the UAT environment, and the same in production? Moving things into production is complicated and error-prone, and it's really done poorly today. There's differences in environments between an individual development environment, a team, and a UAT. It’s kind of a patchwork of manual processes and scripts to make it happen. Sometimes people are copying things or mailing files. It's kind of a hope environment that, “Okay, I'm hoping that what I've done in my individual development environment works in dev, and then I sort of hope it works in UAT.” I'm not proving that thing works ahead of time. And that creates a lot of complexity, a lot of confusion, a lot of waste, a lot of slowness and time.</p><p>What it means is that it's not smooth. It's not a pipeline of moving things from a development environment to production. It's a patchwork and slow process that eats away at the time that people could use to be doing something more interesting.</p><p>Here's an example of a large European telecom company. They've got four environments: a development, a system test, an EDW pre-production, and an EDW environment. It takes four months to move some SQL from one environment to the other. It's a manual process, they have a lot of meetings, they have a lot of checkpoints, they have a lot of documents that people have to do, a lot of review boards. It's not that this team has just got an EDW. They have lots of tools, but just to move EDW code into production is too slow. And so you can imagine your business customer who says, “Hey, I want this new dataset in my data warehouse.” So I've got a source a table, land a table, integrate the table into some dimensions, and I’ve got to be able to visualize that. And it takes four months to get into the data warehouse and another month or two to get the visualization. So six months have gone by. So you can wonder why your business customers sort of roll their eyes at the data and analytic team and keep saying, “Oh, they're so slow” or “I'm going to hire a consulting organization to do the work.” It’s because they have an expectation if they can get a box shipped from California the next day to show up on their doorstep, well, why can't they get a new dataset into their dashboard tomorrow?</p><p>We did a survey with Eckerson Group on how long does it take to deploy changes into production. About 76% of the people were too slow. And you can see the chart on the right. A lot of people take weeks or months to deploy stuff from dev into production.</p><p>I think the other part of this is that when deploying into production, we've got to remember the people and their motivations. In a lot of organizations, there's someone who's a production engineer, in this case, it's Eric, and they're kind of production perfectionists. Their goal is to minimize errors and chaos. Maybe they know a lot about the systems and maybe not a lot about the data, but they get really upset when things go wrong. You look at the people on the other side, maybe a data scientist or a data engineer. They want to create, so they want to do new things. Those are very opposite terms, but yet these people have got to work together. The idea of throwing things over the fence from a data scientist to a production engineer is just unacceptable because the production engineer gets unhappy and the data scientist has a lot of models they can't put into production.</p><p>These characteristics of how organizations have complexities in their environments, slowness in deploying, and the organizational challenges of the different people that work in the different locations, contribute to this challenge that we have.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><u>Transcription</u></p><p>Well, so what can we do about it? Again, here's an aphorism: It's not about a fear of making a change, it's about the removal of fear from making changes. That's another idea of how you change your focus.</p><p>Let's think about this in a very simple way. Think of all the data and analytics you do—the data that goes in production, the factory that happens with your Python code, and ETL code and VS code and etc—as kind of wrapped in a big pipeline, a big tube. Data goes in one side, stuff comes out the other in dashboards or whatever. And imagine there's a stoplight on the top of it that says green is good, red, there's a problem, yellow is a warning. So in those tubes are not one pipeline but lots of pipelines, and everyone who's developing or executing is working in a pipeline. And there's lots of tools going in and lots of production data. Companies may have hundreds of these. We just want a light to say, “Hey, look, things are working in production.”</p><p>A very simple way to think of what we do in terms of deployment and DataOps is to say, “Okay, I've got this production pipeline. I want to reflect that with my development team in a development environment. And instead of production data, I want test data. And I want to be able to run that in a development environment. I also want to have my red light green light on top. And basically, that says, it's still working—the change I've added into that pipeline hasn't broken anything. Then have a safe and controlled process to take that from my development environment with my test data and my developers and put it in production.” And that's really it. We're just trying to make sure that when you have something in production, that you can change it and deploy it in a safe and controlled and risk-free way.</p><p>What is this idea? First of all, fear is a good thing, right? Don't take a lot of risks. But fear is something that you should really listen to. When we wrote the DataOps manifesto, the first three principles were kind of the basis of this idea of how to focus on fast deployment and fearless deployments. The first is continually satisfy your customers. And what that means is don't spend three or four or five or six months building something. Get something in front of the customer quicker—and perhaps when you're even a little bit uncomfortable—so that you can get feedback. Because they may say they don't need it, and then you’ve saved all those months of work. Or, they may give you another direction. Business people or consumers, they need to see and feel and touch analytics.</p><p>Get something working—maybe not perfect, maybe not the best model or maybe not the exact correct data, but get something working, and then change from there. Then the idea is to embrace change. A lot of people spend a lot of time focusing on new features, and they get something in production and they're afraid to touch it. “It's going to break; I don't know what's going on." And instead of embracing change, they're putting off change.</p><p>So these principles about focusing on your customers, getting something working, and embracing change really mean that your team ends up maximizing learning. That's what it's about. If you can iterate quickly, if you can get feedback on something that's working enough, and you can change it without killing yourself or creating a lot of technical debt, then you're in a good place.</p><p>So what are these principles to follow? Well first, create a repeatable and reliable process for releasing data analytics. (And I call data analytics a dataset, a data science model, some schema, some code, some visualization.) And then try to automate things. So what that means is, instead of doing manual processes or having meetings, have buttons to push or scripts to run. And keep all your work under version control. The work that your data scientist or your data engineer is doing, that goes in version control. And even the processes that we're going to talk about here, themselves, should be under version control.</p><p>Here's something that sounds counterintuitive: If it hurts, do it more often. That's the idea. If it's slow and it breaks, try to take things that break and automate them, make them more repeatable. Also, build in quality through automated testing. We're going to talk quite a bit about that, because that's the lynchpin here. There's no magic in throwing code from a developer box into production and kind of hoping it works. Then babysitting the environment and waking up on Saturday morning with panic because something went wrong. But the trick here is to make absolutely sure in development that it still works.</p><p>The philosophy here is trying to get people responsible for the release process. Because done and complete means it's in the customer's hand. Done doesn't mean I've checked it in, or done doesn't mean it works on my box and not another person's box. And to do that, you've got to love your errors. Also measure your cycle time: have a chart that says how fast is it taken for me to deploy.</p><p><br class="ProseMirror-trailingBreak"></p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><u>Transcription</u></p><p>You know, one of our customers has this saying: “Want a real superpower? Have the power to say, ‘Sure we can do that we'll have a first draft tomorrow.’” You know, that sounds great. How could you actually do that, given the complexities of the world? Well, I’m going to give a scenario. Let's say there's a VP of marketing, and he or she has a data engineer and a data analyst working for them. And they're working in the cloud, so the data is in Redshift. Their data work is done in Pentaho, which is an open-source ETL tool, and the visualization is done in Tableau. We're not really particularly bound to any ETL tool or ELT tool—it could be Pentaho or Informatica or Talend or whatever tool you like to use. Likewise, we're not bound to a particular BI tool, and a lot of big companies have multiple versions of these.</p><p>What we do want you to do is be able to do this. So when a customer says something, be able to give them a segmentation tomorrow but do it in a way that doesn't cause your team to get stressed out, doesn't cause a lot of technical debt, and doesn't create a lot of risk or an ungoverned nightmare.</p><p>Those are great words. But what does that mean? Well, in practicality, sometimes when you want to do something you've got to bring a new person in. In this demo, we're calling it a data scientist. And that data scientist has got their own tool, this is a Jupyter notebook, and they've got to do their work. In a lot of cases, a data analyst can just make a segmentation as a calculated field in Tableau. And maybe that's good enough. But a lot of times, you've got to get everyone involved. You’ve got to change the database, add a model, change the visualization. So this whole team needs a place to work. In our software, we call that a kitchen.</p><p>I'm going to show you this process of working in a kitchen, and then how you deploy from a kitchen into production and how easy that can be, and how risk-free that can be. I'm going to go into the DataKitchen product. One of the things that we build in DataKitchen is this thing called a recipe, and this is a series of steps that happen. This is the one that's running in production because on the upper left, I'm in this production kitchen.</p><p>Here's how you build something. So the data engineer’s got something built in Pentaho, and they've run with it. Actually, they used their favorite tool to do this. They've got Pentaho, on their desktop or on a server. And that Pentaho, itself, creates these files called KTR files. These are just XML files that are created by the Pentaho application. You know, almost every tool in data analytics, has an editor and has an engine and has a file that actually stores the work. And so we run these; we actually stick them in a Docker container. But the most important thing is when we run them, we test them. Here we're actually getting data out of the ETL process and doing a simple test comparison against it to make sure that it's right. When the system runs, we actually create this thing called an order run.</p><p>So in production, we ran it, and it was completed successfully. That's good because that means our analytics are working in front of our customer. And if I go and look at this, I see these five are all green. One of the challenges in data and analytics is that as things are going through the system, it's hard to find out that if something changed in the back or broke the back, how it affected the front end. We do that by actually decorating this whole system with a bunch of tests. Here's a test that we run on Pentaho to make sure it ran right. But we're also testing Tableau. We're going into Tableau and pulling out data from it to make sure that it's right. Because if your data provider gives you poor data, how do you know, then, that it's not where the focus of the problem is? There's logic or if-then-else or code in every one of these nodes in the system, and that's why we test.</p><p>We have a concept in our software called a kitchen, so I've got to create that place to do work. Here, I'm in the production kitchen, and I'm going to create a child kitchen. To do development and analytics is kind of hard, but the environment that you do work in is actually kind of interesting because it's got a lot of components to it. An environment can be spun up quickly and shut down, like in Amazon. Or sometimes you've got a fixed server, and you want to create a place in it. You want to invite people into your environment, maybe some of your team. You want to be able to run recipes, and then not wake up your production team if there are a bunch of errors. So you want to build a little, test a little, learn a lot. Because these recipes are complicated, you want to just zero in on the piece that you're changing. In our software, there are a bunch of variables and controls that allow you to do that. Finally, because you're making changes and you need to collaborate with people, you need to branch—meaning you need to have a repository of all your work and create a virtual copy of it, that then you can make changes and merge back into the copy that it came from. All these things actually go into creating an environment.</p><p>So I'm going to give this kitchen a name. I'm going to call it Dev_Sprint. And I'm going to hit Next. In our software, there's some work in how we set up and configure setting up the environment. This was done ahead of time. I'm going to create a schema here, and I'm going to pick just one recipe to work with. Another aspect of the environment is the authentication credentials of how you log into different things, and I'm going to pick a default here. Then I'm going to hit Next. So now I've got to be able to go in and create two spaces in two different servers. One is Tableau, and the other is Redshift. And I'm going to hit Complete Wizard. So I'm creating the environment that can do my work in. In some organizations, those servers are already running. Sometimes this takes months in some organizations just to set the development environment up. But we think that creating a development environment should be quick and easy; and that when you go into your kitchen list, you can actually see that this production kitchen has got a child kitchen, where people can do their work.</p><p>I go into this Dev_Sprint kitchen, so now I'm in my development environment. How does the development happen in DataKitchen? Well, someone goes into a recipe, and they edit it—I've edited it ahead of time here just to save us some time—and they add a node into our graph. The data scientist added in their Python Anaconda node, and it works kind of just like the one I showed before. Remember, they're using their favorite tool, and here's the code from the iPython notebook. So here's our k-means cluster that it ends up running in the same test framework when it runs.</p><p>Here's a key thing that when you want to be able to help people deploy quickly from dev to production, they've got to be sure that when they're running in development that things work. So to do that, you've got to be able to run these recipes, or run the systems, in a development environment. The idea here is that if you run in the development environment, you need to be able to prove that what you have newly created works but also prove that you haven't mucked up anything else that already is running. The way that we do that is: we've got these systems that branch and merge, and so I've stuck in these two Python Anaconda segments and load segments. How do I know that what I've done here hasn't messed up my Tableau reports? One way to do it is just have the smartest guy or gal in the company go in and look at it and sort of give it the blessing, do a Technology Review Board. But the idea of DataOps is don't use your best resources to kind of give blessings to design. Be able to prove that when you make a change, that change doesn't affect anything else that's already working. So number one, prove that what you've done works in development, and two, prove to yourself that you haven't broken anything else.</p><p>The way that we do that is you run in a kitchen. If I drill into what's being run, you can see that this recipe is progressing. Each one of these steps is going. Then if you look at the actual test results, I added in this new feature, this Python Anaconda segment. Yeah, there's one test and we could do more, but I proved to myself that it works by that test. So yeah, my new feature works. So am I done? Well, no! How do I know that I haven't inadvertently broken something else? You need to rerun the tests that exist in production, at least some of them. Here I'm rerunning this Pentaho test, the one that I ran before. I'm rerunning the Tableau test to make sure I haven't broken anything.</p><p>This is a way that you can get confidence that in development, what you've done already works by testing—and by testing not only what you've done, but testing every part of the system in development. That can give you the confidence. We guide our customers to say 20% of your work should be these automated tests because you can run them in production, and they help you find out if there are errors before your customer sees them. But also in development, they actually function more like regression and functional tests.</p><p>So let's see this case here. If I went into this demo, and I made a small change—let me just go in and change this description. I’m going to change it to “webinar” and hit update—and I'll show you how we get the code from what we've done in development up into production. There are different ways that organizations handle the promotion of code. The simplest way is to have code just move from a development environment into production, and then next time production runs it picks it up. There are other cases, and we'll talk about this, where people have more stateful infrastructure. You've got a development environment and you've got to a UAT or a pre-prod environment and a production environment. You've got to do deploy scripts or patch scripts to make sure that you've got the right DDL, the right store procedures, the right code in each place.</p><p>We can handle both, but I'm just going to show you the simplest case here of how you merge code from Dev_Sprint into production and make it available. So here, you've actually got to take the code and merge it. What that means is since we believe all analytics is code, you've got to be able to go in and see the changes. Here I made a really dumb change that said “webinar” versus “today,” because I already had this made out. This is that technology that we've wrapped called Git, which is a version control system. So I've done the merge, I can manage my order runs, which is a way for us to manage statistics. Then I hit "Complete Wizard.” Now I go back to my kitchen list, and if I go into that production kitchen I can see in the history that this has changed. Therefore, this really dumb code change is in production, and it will be run the next time the systems run.</p><p>So what we're trying to do here is make it safe and automated to take code from a development environment into production. Not have Technology Review Boards, not have a whole lot of documentation and meetings. But sort of give people these buttons to push to make sure it's automated and testable and that there's artifacts that are created that can you can tell what happened.</p></div></div>

<div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><u>Transcription</u></p><p>That's our goal in data kitchen is to make it fast and automated so deployments don't take months, they can take hours or minutes. And that when you deploy, you have a very, very low risk of failure that you have a high rate of success.</p><p>Everyone sort of believes that it should be fast to get work from my developers fingers into production quickly. We all have our software people next to us talking about this thing called CI and CD, continuous integration, and deployment. They're talking about how fast they can get their JavaScript code from their box into production. I've seen this pattern in a couple of organizations that implemented automation of deployment and unit tests and still had lots of problems deploying things from a development box into production. So this section is really about why CI and CD and unit tests are not enough, or how to go one step beyond thinking about what software developers do and apply those ideas into the world of data analytics.</p><p>We've written quite a bit about the differences between DevOps and DataOps. In a DevOps process, you're developing software, you're going through a build process where you're assembling all the artifacts and compiling them. You're testing them, and then you're deploying them perhaps to a web server. And then you're running. And so this circle, continuous integration, is what happens in the development process. I check some code in and it runs a bunch of tests and makes sure that the system is still in good shape. In some organizations, they go a bit beyond that. I press a button, and then it automatically deploys into production.</p><p>But this is CI and CD. In DataOps it's a little bit more complicated because you've got this big deal in terms of managing sandbox. You still have the same development, but we've got a lot more tools to orchestrate. Similar testing, similar deploying, but we also have orchestration and monitoring. So the DataOps process is a little bit different and a little bit more complicated than a DevOps process. In a lot of ways, what makes DataOps different than DevOps are these ideas of the complexities of creating sandboxes, the orchestration of all these different tools and data and analytics, having testing and monitoring—we're going to talk a little bit about that the dual roles of testing and monitoring—and then finally collaborating. Because a lot of organizations, there's not one data and analytics pipeline, there's multiple ones.</p><p>Let's talk about testing. There's this idea that tests have a dual nature. If you remember at the beginning, we said in production your code is fixed but the data is varying. So when you're writing tests, you're monitoring the variation of the data to make sure that your customer doesn't have a problem. In development, your code is varying but your data is fixed, because you're using a copy of production data or a test dataset. The purpose of those tests that you create in development is different. Instead of trying to monitor to make sure your customer doesn't yell at you, you're trying to do regression or functional or performance tests. And it turns out these tests actually can be reused in a lot of places. Maybe not 100%, But the tests that you run in production can be reused in development and their purpose changes.</p><p>The idea here is that in development, you should really—like we showed you—press a button that says I want to be able to deploy the feature. But before you do that, there's a whole set of types of tests that you want to think about writing.</p><p>In software, they've got a whole nomenclature for tests, and some of them apply, and some of them don't. A lot of organizations do something called a unit test. The unit tests test their ETL, which means: I create some fake data, I run it as an individual unit, and I make sure that the logic is right. And that's good. I mean, it's not bad to have a unit test. It's just not enough because you need functional tests that look at things as the whole feature development. If a feature is composed of six units, well you want to check all six units together. Then you need a regression test, which is running the same thing over and over again. Then even down to something called an end-to-end test where you want to test the system as a whole. Each one of these types of tests has a different purpose, but the idea is that you want to make sure that you have a breath of tests.</p><p>We've written quite a bit about different types of tests that have to do more with historical balance tasks, location balance task, statistical process control tests that apply more to the data world, but these concepts apply. The point is that testing an individual unit, of itself, isn't sufficient to prove that when you made a change, everything works. You've got to test the whole system.</p><p>If you take that as true that you can't test just a piece of it, you got to test the whole system—well, the problem is that system is actually built based upon the organizational structure that the company has. So you may have a production team at the top here on the left and data engineers and data scientists working in it. And you may have your own development team structure reflected in your pipeline structure. It gets complicated when you've actually even got self-service; you may have remote teams on the left that are pushing things to prod and getting it separately from your enterprise production team.</p><p>And so that's a challenge, right? Because your business customer kind of sees the sum of all these pipelines. But the pipelines are owned by different people. Even how they work, the development environments, the kitchens they work in are different.</p><p>So how do you actually deploy things to production in this world? Well, I think with a platform like DataKitchen, you get a unified way to address all these things in a commonplace. That's one way to do it. In some organizations, they've got CI and CD working with a DevOps team. So on the software side, they may have a dev environment, a test environment, production environment, and they may want your data and analytics team to actually fit into that environment. You just need to be cognizant that the organizational structure of your data and analytics teams actually fits into the pipeline execution.</p><p>The idea here is that there's a structure to your teams represented in the pipelines and that that structure needs to follow how you deploy into production so it can be faster. But in some cases, you've got to actually work within a within a dev team. And then in the final case, some organizations on the left have stateful environments. They have a development environment, a test environment, a production environment, and each environment has state—it's got data in it, it's got logic in it. So you’ve got to patch environments as you move things up. That creates complications.</p><p>What I've seen some teams do is they actually work to a more of a functional architecture, where there's an unchangeable dataset. If their data is small enough, they can actually rebuild the entire database from scratch. That way, that functional architecture makes it easier to deploy from one environment to the other. But I think deployment in both—whether you've got a fixed development environment, which is more of a stateful system, or you've got more of a functional architecture—works the same.</p><p>I would like to talk about two more items in terms of deployment. The first is a data architecture consideration, and the second is a measurement consideration. Everyone's seen a data architecture where you've got a production environment with a data lake and some data engineering and refine data. Data on the left and people on the right, and this is pretty common. If you're an architect, you really want to think about how you deploy this automated deployment. It’s a very central thing, how you move things from dev to test to production. Think about the right to repair, the right to change, how you automate deployment—not as an add-on—but as a central part of how you work. I think that's reflected in how people develop software nowadays. In a lot of cases, when people build new projects, they work first on these deployments. They work first on the factory, and then on the things that are made for by the factory.</p><p>Second, if you're going to make a factory—you're going to help automate deployment—you’ve got to be able to measure the effect of that. In DataOps, we guide people to measure the process. Things that you want to focus on measuring are your team productivity, how fast they deploy, how many environments they are running, and even the code and test coverage. So what that looks like in our software: if you take a particular project—and I'm going to start at the upper left-hand side—you can count how much collaboration has happened by just counting the number of kitchens that people have used, because a kitchen is a proxy for an environment. Then you can count how many deploys happen from a development kitchen to production or from a feature kitchen. If you see more deploys, that means people are getting more value to production quicker. Even look at the risk involved. If you look at this is over time, looking at the number of automated tests going up in aggregate on this project.</p><p>There's a graph here that has one thing called a ratio of the number of tests to nodes. Does every node have a test? Does it have coverage? How many nodes are there in a recipe? How many recipes are there? You can then get an idea of the coverage and complexity of the software that you're building. If you're a manager and you've got 15 projects, you've got 20 or 30 pipelines in production. How do you control the work and get some idea of is one team doing good work or not? Is one team deploying quicker the other? That's why you need to measure it.</p><p>So the last aphorism I'm going to give you is: “Multiple takes lead to highlight reels.” And I think that's the idea here of why you want to do it. You want to keep trying. And if you keep trying, you're going to get it right. And what we hope is when you apply DataOps principles in our software, that your deployment latency—how fast you can get something from dev to production—can go from weeks or months to hours or minutes.</p></div>

## DataOps Environment Pipeline

<div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>Underlying all this is this box on the right that there is an environment that you do this work in. That environment encompasses a lot of different things. Because if we're going to talk about production, well, there's production in what? Deployment, it's from…to, from what to what? Environments are actually foundational to all these pipelines, so we actually think of this as a pipeline itself, that building and using these environments can be amenable to this idea.</p><p>We're going to focus on the environment pipelines. Why is this a problem? Well, first of all, we did a survey with Eckerson, and we just asked people, “How long did it take you to create a new development environment?” For most people it’s just way too long: Some of them are months, some are days, some are weeks. To give an analogy, if you look at a software team—when a new software engineer joins the software team, high-functioning teams want to have a development environment set up for them or for them to set up their own development environment within a day. And then you also want to have them be able to deploy some minor code fix to production that week. How many analytic teams have that same case where a person can get a development environment, successfully make a change, and deploy it to production on their first week of work? I think that's possible here to do with DataOps principles, and I actually think there's business value in it. And people aren't doing it.</p><p>The provisioning of an environment, the creation of it, is a slow and manual and pretty high-touch process. It's a leading challenge for analytic organizations. Because when there are differences between a development environment between a production environment between one cloud environment and another, they end up not being fit for purpose. And to be able to reconcile those differences is a manual slow and pull-your-hair-out kind of operation that leads to outages and instability. It leads to expense, having more hardware and software, and infrastructure and licenses than you need. These differences between data in the environment—test data and production data—can cause deployment errors. This translates into impact on quality data and analytic organization, overspend delays in your projects, and slow delivery of changes to your customers.</p><p>So if we look at this, and I showed this in our last case, the columns here are environments. There's an individual development environment here, a team development environment, a test environment, and finally a production environment. If we go down to the last box, what are you trying to do in each one of these? In your individual development environment, trying to develop a Python model or something in Tableau. In a team development environment, you're trying to see how that fits with what everyone else does on your team. Then in the UAT environment, you're trying to run it to make sure it's not going to break before production. And the hardware is different: You may have development servers and test servers and production servers and databases. There may be data that's different: You may have a test dataset, an individual development environment that's very small, the team environment may be a cleaned, 90% version, the test one, maybe 100% version, and then finally production environment. And there are different libraries and tools that run on each and different servers. And then there's data that's in each one of these that's different.</p><p>Anytime you have a case where there's a different version of a Python library between an individual development environment and test environment, there's differences in test data and schema, it's an opportunity for something to break. And those are really sort of hairy, annoying problems. You know, they're just complicated.</p><p>What we mean by an environment here is a little bit of a bigger term. It's not only the hardware and software that you use to run your analytics. It's the test data that goes into that. It is the right libraries of the hardware and software. It could be the network configuration that goes with it. Also, the code branch that you're working on, and all the tools that you're working with, even the people. So the environments—they're complicated, and they're in some ways more complicated than a software engineer because of the size of the data, the number of tools, the chain of the tools, the distribution of those across the organization, and the security concerns. So it's just hard to get all these things together to build an analytic environment for you to do your work.</p></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>In our software, we've thought of this thing as a kitchen that brings together all these pieces—the hardware and software and test data, the people who work on it, the recipes that you're working in and their git branch, the ability to work quickly, a lot of parameters (because you may want to run it on big data or small data, you may want to run the full process or a partial process), the git branch, and even connecting to whatever kind of agile sprint tool you're using.</p><p>A challenge in that and building the environment is not only to be able to treat your hardware and software environments as pets, not cattle, to be able to instantiate them from creating code; There's also a real challenge in creating test data. Because test data, itself, is hugely important. In some organizations, you can just copy production data. And maybe it's small enough, or maybe there are no security concerns. For many organizations, however, you've got this challenge in the distribution of the data. How fast can you get your test data from production? I've talked to organizations where the test data is six months out of date from production. That means there are schema differences and distribution differences that happen in the data that all makes an opportunity for error.</p><p>And then quality: Is the data that you're using, like production? Have you created fake data that doesn't have the same attributes in it?</p><p>And then security: There are GDPR rules, there's California privacy rules, there's risks associated with it. If you're dealing with consumer data, have you stripped out social security numbers or names or healthcare data? In some organizations, the differences between production data and development data are profoundly different. What we're trying to do is minimize errors to be able to make it happen and also minimize costs, because in some organizations we've talked to they have a 10-node development server that's costing them 30,000 a month to run on Amazon. Does that need to be up all the time? Does that need to be full of data? What's the expense in managing that? Also, how do you manage this from a privacy and compliance standpoint? So there are a lot of challenges in building and managing test data.</p><p>Let me talk about just the very basic difference of how we manage environments—not multi-cloud or multi-organization—just development environments to production environments. Let's take one simple case. In every organization, there's someone like Eric, who's your production perfectionist. They run the production environment, and they've got a little bit of skill in everything. Their goal is to protect and perfect the daily grind of delivering data and analytics. They want to minimize errors. They tend to be taskmasters, and they're right to be taskmasters because they want things perfect. Because when things go wrong, they're the ones who get yelled at. We think that in using our software there's someone called a DataOps Engineer, and their goal here—and this is Chris—is trying to optimize the operations from dev to production. They've got DevOps skills and cloud skills and skills in our software. Then there's a whole different set of data doers, maybe it's a data scientist or a data engineer, someone doing BI, but their goals are to create new features for customers. They want flexibility, and they may have a whole bunch of different tools. And I'm only showing one person here—a woman called NoProd, because she has no production access. But there tend to be dozens of them in organizations, and they have different environments.</p><p>So production is managed by Eric. And that's separate software. In this case, it's a secure environment. So there's no access by developers. That's not always the case. But in some financial services companies, they don't want developers to have access to production data. They don't even want developers to have access to the network that production is on in some cases. So it's a completely encapsulated environment. Maybe has its own VPC in Amazon or its own Kubernetes cluster.</p><p>We need to be able to build something in development and push it to production easily. There has to be a different, what we call an agent in our system, a different way to run something in development than in production because they have separate hardware and software. They're secure. And they have different access levels. They have different management teams, and they have different credentials that go in. So all those things are what we're going to talk about, with how this interaction of people and technology works.</p><p>If you look at the production environment here in this example, there's a VPC that wraps our DataKitchen agent running there. There's some SFTP servers, a Redshift database, an S3 database, a Slack channel for alerts, different places to put Docker and Python, and there's different development environments. Here's a very simple case of just trying to load some things in a database table. You've got all these components that you've got to be able to organize, the people and their technologies so that you can move things quickly from development into production.</p><p>What's key here in this diagram is these things that are in green and red. These are what we call variables in our system. They're pointers to what the configuration is. For instance, you're going to see DK_Implementation_Dev and DK_Implementation_Prod. They're pointers to variables in our system. And those variables can be set differently.</p><p>The scenario that I just want to show you is, okay, we've got a VP of marketing, and they say, “I want to see some new profitability data ASAP.” So how do we get that from dev to prod easily, with low errors and low ability to cause regressions into production? What that looks like in our software is—and I’m in DataKitchen here and I'm going to this kitchen UI. We've got a production environment here, and I'm logged in as this woman, No Prod. She doesn't have access to production. She can't get into that kitchen. She doesn't have access from a network standpoint, doesn't have access from a DataKitchen standpoint, but she does have access to this demo kitchen that she can work in. So I just want to show you how we've defined a kitchen.</p><p>If I look at the recipe that we want, that she's working on, it's got a bunch of nodes that do different things, some of which are Redshift. Here's a Python pandas transform. Here's S3 to Redshift. And these numbers represent the number of tests that happen in each node.</p><p>But let's go and figure out exactly how we set up the kitchen in order to make this happen. Because when I'm doing some work in Redshift, I need to know for instance, what the Redshift password is, and what the Redshift database schema is. How we set up a kitchen, it's got a bunch of different parts to it. The first part is the users that have access. You can see this woman, No Production access, her email is right here on the list of people. Then there's a set of what we call overrides, which are the way to abstract out the environment that you work in. We've got a bunch here. If I look at this one, which is our Redshift configuration, here's the name of the database, the IP address of the database, and the password—which is actually a vault credential—and the port. Why is that important that it actually is a vault? Well, it goes in and says this is the password and Vault is a server that stores encrypted passwords in our system. When we pull things from it, we're pulling from it in the context of the dev demo dev kitchen. So we're getting the right password in development. And then in production, we're getting the same version of the password based on a different vault system. Here, you can see that we've got configured a different agent, which is running in that environment, and different alert configurations. So all these things go into defining what it means to be a kitchen.</p><p>The basic goal is that when you're doing some work in the kitchen, and you do some development in this Test-Merge kitchen, that when you do the changes up from Test-Merge into the demo development environment, they're easy and fast and no errors. Abstracting your environments is an incredibly important way to do that. Because that lowers the opportunity for errors. So defining your environments upfront is part of what we do in our implementation process.</p><p>So let's go on to another example. A lot of people are trying to take advantage of the Cloud and use the resources. So they've got either an on-prem, and then they're in Amazon or Azure, or sometimes they've got multi-cloud. Here's an example of what we've worked on with one customer. They were in both Amazon and Google. They were doing part of their work in Redshift and Talend and Python and in Amazon, and then part of their work using GCS buckets and BigQuery and then in Python. There were data engineers on one side and data scientists on the other. And there’s a story about why they have two, but you could take this and instead of AWS Cloud you could put on-prem and see the same point. The point is that they're two completely different environments, but the customer gets the result at the end. The analytic is produced by the combination of both those. And when you have that model or visualization, how do you make sure that each part of the process is coordinated? Also, since they're separate teams, how do you allow the separate teams to have different control over each? The one way that we do that is to make sure that these circles, themselves—the AWS Cloud and the Google Cloud—have different agents running in them and have different variables and overrides that find what the environment system is. So we have variables that say, What's GCS? What's Python? what's S3? Then the third part is we actually allow users to embed recipes within recipes, so they can work together. We call those ingredients.</p><p>So the idea here is that you can have an overall recipe that says, “Run something in AWS, then run something in Google Cloud” while testing across all of it to make sure that your customer does. And, having each individual team in their own individual environments manage these sub-recipes, is a way to have both freedom and control at the same time. That's one example of multi-cloud recipes.</p><p>The next example I want to talk about is multi-organization environments. This can also be challenging because a lot of us in data and analytics face the challenge of how you deal with self-service. In this case, there's a centralized Home Office team that uses SQL Server and Python, and they're in Boston. Their data warehouse is updated once a week. Then you've got these local teams that use Alteryx and Tableau around the country. They're used to doing what self-service teams do: they're being very rapid reacting to what the customer needs. But again, the customer sees the result—in this case, the combination of the result, the data, and the models that are put in the database, and then the actual visualizations and data prep that happens.</p><p>So again, we've got this similar challenge, right? If I change a schema, how do I know that I haven't broken a bunch of reports? Or if I'm a self-service data analyst using Alteryx and I've mixed in a small dataset, how can I make that dataset available to everyone? How can I make sure that across all my different Tableau reports that the calculations are correct? And just how do I make this work when I've got some new data and a schema and an updated report? How do we coordinate across this; because the home office team is running in their data environment and the local office team is using Tableau server which they pay monthly, and they're using Alteryx desktop.</p><p>The same principle applies: Each team is going to have their own recipes that have their own shared responsibilities that run in their own environments. And then they have to coordinate across each in the same idea that ingredients work. And so when we get a request from a customer that says, “Okay, I want to see my new target accounts,” the first line of defense and the tip of the spear on analytics (and I think one of the primary benefits of self-service) is that that self-service team can then create a target account Excel file, load it into Alteryx, and mix it into Tableau, and show the targeted accounts layered into the existing data. And they can do that in a few hours, and get feedback on it. What we want them to do is be able to create a local office kitchen for them to work based on the current version of the central data, do their work in that, see if that work actually provides value to the customer, add tests across the new data that they put in and the new columns they put in Alteryx and Tableau, and then be able to take it and deploy that back into production.</p><p>Why does this all matter? And why would they even bother to do it? Well, it helps the home office team then know, “Well, I've got a new file that I've merged in a new Tableau workbook and your Alteryx workbook that are put into production to manage that, and anytime I run something and have a change based on what's in production, I can then run tests against it.” So this is a way to have a home office and local office work together, to have code that's aligned, but also have environments that are aligned for them to work together.</p><p>So let me go through one more case study. And this is sort of the same theme, but a different idea.</p><p>In self-service, there's a top five bank, and they really want to reduce the risk in self-service. They've got like 1000, non-IT users who want to use data to get insight from it, but the provisioning of the data that they need and the monitoring for compliance and risk is a huge problem for them. So they want to be able to say, “Here's some short-term use, here's some data that you can use, here are some tools, but I want to make sure that you've got the right data, that you have it for the right amount of time that we know what you're doing with it, and that I can service those requests correctly.”</p><p>Because you're giving data about your customers or your suppliers to these people, and how do you know that they're not doing something that goes against policy? So be able to create a data sandbox or a self-service sandbox for all these people. It's a way to manage risk and apply compliance and actually just help the IT team out from doing this work. What does that look like? You've got a big bank, and they've got these centralized IT and data resources. Maybe in Brooklyn, you've got somebody doing small business loans in the New York area. Maybe in Texas, you’ve got someone doing high-net-worth wealth management and trying to do some analytics or get some data. Then finally, you may have some Northwestern branch regional marketing. So you’ve got these three different people or three different groups against a big financial service company, and they have different needs. So for instance, the guy in Brooklyn may need business data for last 24 months. So you need some DMB files loaded in a SQL database, and you need Tableau access. So the guy in Texas may need Texas customer data. It's got to be loaded in a database. But he’s more technical: he wants Python and SQL access. And finally, the Seattle customer bank may need the DDA data, but they want to anonymized. And they want to access it with Power BI and have some sort of data dictionary understand what's going on.</p><p>So there are three different teams, three different uses of data, three different cycle times. But they all share this common set of challenges of what's the logistics of giving people their data and tools? How fast, what's the cycle time that I can do it? Are they allowed to use it for how long are they use it? What use case? Are they allowed to use it? And can they monitor that? Can they save their work? Can the IT team revoke access? If the people leave the company, can they reuse what they've already done?</p><p>And so self-service from an IT perspective, means fast, low-cost support and management of risk. How does that look? Well, we actually helped set up a process for them where they could walk through a state of making a request and approving this analytic development environment. It takes time sometimes to get these things ready—Are they in error state? Is the user done? And we used a form front end for people to request in a simple way. Then we actually enabled it with a bunch of DataKitchen recipes—some that did decommissioning, some did provisioning, some that did dispatching, some that did monitoring, some that did failure detection. And all these different parts need to run in different cycle times and different work based in DataKitchen.</p><p>The benefit here is instead of spending three to four weeks to manually go through a huge IT checklist to build all this stuff and with different times and people, it's been automated. This automation allows them to monitor what happens and be able to automatically revoke access, reuse their work, just take a lot less time, and be able to manage this task support and risk.</p><p>So that's the use cases: a development to prod, which is the basic case, different cloud or on-prem-to-cloud environments, multi-organization environments, and then a case of self-service development environments.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>So let's go through the last part in terms of principles and architecture and measurement considerations across anything having to do with environments. We wrote this DataOps manifesto, and the 11th and 12 points were really about making environments reproducible—being able to say, “I want to quickly reproduce an environment from a development environment into production, or from production into development, and make it perhaps disposable that I can recreate it.” And this idea of reproducibility and disposability of environments is really key.</p><p>There's this phrase in DevOps: “Treat them like cattle, not pets.“ There's a lot of treating of environments like pets, a lot of manual processes, a lot of “go through this checklist, talk to this person, they know how it works.” So the vision is you want to have a button to press that says, “Give me an environment and have all that be automated.” The worth here is that then you can start solving that case of the 22-year-old with a CS degree who joins the team. They can have a button that says, “I want to have an environment.” They could do their work in that environment, then they could run tasks and help deploy into production. It isn't so far-fetched to believe in these principles and to make it happen in your team.</p><p>Here’s some core principles I think everyone should look for. Know what your environments look like, first of all. Know what a development environment is, what a production environment is, and be able to coordinate and communicate activity across those environments. Then, all these tasks that go into building and abstracting environment—automate those. They're pesky, they're annoying, but they have real value. Put some good engineering resources on them. Make sure that this process of managing environments is, in itself, a source of data for you. And of course, keep everything under version control, and even keep the scripts that you use to build the environments under version control.</p><p>If it takes time to build these environments, do it over again and do it more often and bring the pain forward.</p><p>Why is all this? Because there's always a challenge in technical work of what done means. For us, I think done means it's in production. Done means it has gone through whatever number of environments you have—Dev, QA, pre-prod, production—done means in production.</p><p>Of course, give time to test data management. It's a real thing.</p><p>Let's just talk a little bit about architecture considerations. Everybody has a production environment that they work in with a whole bunch of tools. So when we think of DataOps as an architecture, we think of automating deployments, orchestrating and monitoring, and especially on the right here, environment creation and management. These are very important things to do. And as part of that, you want to be able to make sure that you can identify where to run the environment. You need a place to say, “I want to run it in Dev, I want to run it in test, I want to run it in prod.” Make it easy to say, “I want to run this Python code, this Tableau workbook, this SQL code. I want to run this in development, now can I take the same work and run it in Test and make sure that everything works?” Think of it as a hub and spoke. You want to have a hub. Then the spokes are each one of your environments. The environments could be different as a data center with a dev environment that's separate from a production environment. Or it could actually be literally different data centers.</p><p>Here's an architectural version of what we showed in these multi-team or multi-cloud cases, where there's one cloud or on-prem environment and another, and there are different agents that DataKitchen is running in each one of these. You've got to be able to say, “I want to run it in Google Cloud Dev, or I want to run it in Amazon Test,” and be able to run which part of it and then have each system call.</p><p>So there's a dance and coordination of these environments that you need. You also need that single pane of glass across them. For a lot of companies, they may have some stuff on-prem, they may be moving to a Cloud, and they have no way to tell what's running or what broke. Being able to see it across the systems is an important way to gain efficiency and time back.</p><p>Another point is to think of it from a process analytic standpoint. How many environments do you have? How many are you using? How many are people creating and destroying? And how fast are you deploying between these environments? I think those things are very basic ways of doing it. Of course, there are more advanced things like, how much is each environment costing you to run if you're in the Cloud? Is there a way that you can turn them on and shut them off quicker, so they don't have to run over the weekend and you're paying all the Amazon charges for them? That's another type of metric that you can create.</p><p>Just to finish up here, we talked about why environments are valuable, why abstracting your development release, one Cloud versus other, centralization versus local environments can really add value to your organization. And, we gave four use cases on that. We talked about some principles in managing environments and architecture and measurement cases. So finally, let's just talk about why this matters. What we hope is when you apply DataOps principles, that your deployment latency—how fast you can get something from dev to production—can go from weeks or months to hours or minutes. And that you can have low errors. My own personal hope is that your data and analytics team is just happier and more productive.</p><p>So if you're interested in DataOps and these four things to focus on DataOps—decreasing cycle time, lowering error rates, improving collaboration, and measuring your process—well, we've got a software product. If you're not there on the software product yet, we've written quite a bit about it. We wrote a book, we wrote a Manifesto, we've got a blog that talks all about these things and kind of gives you the background and justification.</p></div></div>

## DataOps Implementation

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>I guess a lot of people have databases that are kind of black holes. They're dastardly dark databases. They don't really know what's in them. They don't understand it. They have a lot of tables, it's sort of a black box to them.</p><p>And then, they have the data teams have a lot to do, right. Their customers are very demanding, things are breaking. So teams are stressed out and then obviously there are embarrassing production errors, that your customers find and they find them before you.</p><p>And then data teams don't, aren't able to kind of have the business knowledge, you, you may have 1000 tables. How are you supposed to know all about the 1000 tables? So, if you're gonna test data, how can you test data without really connecting the business end, and the levels and data, and the syntax and semantics and pragmatics of data to put it together?</p><p>It saves a lot of time if you can give the pulse of what's going on with everyone else in the organization. And, so I guess the question is what's happening in your system, the sort of living breathing data moving from place to place something happening with data results being pushed to customers. This is good; sort of living and breathing and going on.</p><p>And so for us, we've kind of boiled it down to something called a Data Journey—a path from data source to customer value.</p><p>The cornerstone of this is that you need an expectation layer on what is happening with your with the production of analytics and expectations could be on logs, could be on errors, could be on metrics, they could be on data tests. And so you need to judge what is versus what should be.</p><p>And so these journeys themselves have a use in development, and being able to do development regression testing in addition to production. And then finally, this idea of your data journey and having a place to put it actually makes it really useful, right? One is that you can share it, right? Everyone wants to know the production schedule.</p><p>And, so you know, the way we think about it at DataKitchen is we we want you to focus first on the Data Journeys. So, pulling all this information together and building the Data Journey layer is is what our our software is meant to do.</p></div></div>

<div data-v-87e1d9c8="" class="rich-text-content inline"><div contenteditable="false" translate="yes" class="tiptap ProseMirror"><p><strong><u>Transcription</u></strong></p><p>Now I'm going to talk a little bit about each phase. Really the best way to get started, as I said, is in phase one, and that's to focus on errors in the production cycle. Currently, most data teams spend way too much time finding and fixing errors according to Gartner—78% of their time. And in reality, that ratio should really be flipped, and more like 80% of time should be spent on delivering business value. In our recent data engineering survey, 52% said that errors are a major source of data engineer burnout.</p><p>The number of data errors that teams are dealing with are really staggering. We did a survey in 2019 with the Eckerson Group, and 79% of companies have way too many data errors. We define that as more than three a month. So 79% of respondents had more than 3 errors a month, a whopping 30% had 11 or more errors a month. Numbers like this can have a really huge impact on a data team's productivity and can dramatically reduce trust in the team's product. Now, if these errors could be reduced or eliminated, imagine the huge productivity gains that teams could experience.</p><p>Also, it's not just about data quality. Data quality is just one component of error rates. Quality is really about how your data looks at one point in time. Other issues can cause errors like code changes, processing issues, late data. You may have heard the term data observability, which is the new trending term for something that DataOps has been advocating for a long time. It's a set of practices that enable low errors. DataOps does a whole lot more, but making your system observable is really a good place to start.</p><p>Coming back to your production or value pipeline, you really need to think about your pipeline like a factory. Much like a factory that produces cars, data or raw materials come in one side, they go through multiple processes and transformations, they are touched by many teams and tools, and value is delivered out the other end into the customers hands. So a number of steps have to happen correctly, for it to deliver value out the other end. If any one of thousands of things are slightly off, the analytics can be negatively impacted and the team is held accountable.</p><p>The solution to this is automated testing. At every step in production, you need to add tests that answer questions such as, are your data inputs free from issues? Is your business logic still correct? Are your outputs consistent?</p><p>With DataKitchen, you can add automated tests at every step in your pipelines. The more the better. These numbers here in the circles represent the number of tests at each step. And there is a direct correlation between the number of tests in the pipeline and the number of errors experienced. So as tests go up, we see the number of errors decline dramatically.</p><p>This is not something you have to do all on day one. You don't need 38 QA tests on day one, but it's something that can be built up over time. Every failure is an opportunity to add more tests and build up the robustness of your pipelines.</p><p>Furthermore, when using a DataOps platform, it's easy to get started because tests can be written in users’ tools of choice. There's no need for anyone to learn new languages or tools. If your data scientist likes Python, they can go for it. If someone else likes to use SQL, they can go for it in those tools. And there are an endless amount of tests that can be added like statistical process control, location balance, and historical balance tests. If you'd like to learn more about the different types of DataOps tests, we wrote a great white paper on this topic, and I highly encourage you to check that out.</p><p>When setting up any of these tests, you can also set alerts that will notify you via email, JIRA, Slack, or any other collaboration tool that you're already using if something goes wrong, so that you can get ahead of the problem and fix the errors before your customers find them.</p><p>And then you can also classify errors by severity level and set failure modes. For the most severe errors, you might want to stop the pipeline. Others may just require a warning and further investigation, while others may be informational and just monitor routine pipeline activity.</p><p>So with very little effort, you can implement a really important principle of DataOps and eliminate production errors. When this was implemented at Bristol Myers Squibb, they went from no tests per build to 1000s of tests per build, and from frequent errors per build to zero errors. This was quite an improvement.</p><p>And this really flips the narrative: by spending less time finding and fixing errors the team has more time for the important work of innovation and delivering business value. This also leads to greater productivity and trust. In the words of one customer who implemented Production DataOps, “we reduced errors to about one per quarter. It's now been several years since we've had any major glitch. This has dramatically increased the efficiency of our data team and the confidence of our end stakeholders in the data.”</p><p>Once you've improved your production cycles, and built credibility in the data, what's next? The next question is to ask, how can my team start delivering value faster? How can my team respond to frequent questions from as business customers and start iterating on results with them?</p><p>The answer is to move to Phase Two and focus on your development and deployment processes. Here you want to focus on automating as many processes as you can, which will in turn lead you to maximize analytic development velocity, minimize deployment risk, and also have a huge impact on collaboration within your team or across teams. And I'll talk a little bit more about each of these.</p><p>Coming back to your development or innovation pipeline, this is where you need to start thinking like a software developer. And the key is automation. How can you automate the movement of deliverables from one environment to the next, especially considering that most deployments happen across diverse teams using diverse tools, meeting the needs of diverse customers?</p><p>However, the reality is that manual error prone processes are the norm. In this real life example, it took four months to deploy to production. New analytics involved many different teams and tools. The data progressed through four stages to get to production—from dev to test, to pre-prod, and finally to prod. These processes were manual, which introduced lots of complexity, slowness, and errors. In fact by the time new analytics were launched, the end users sometimes couldn't even remember why they asked for them. That may be a situation many of you are familiar with.</p><p>Overall, our survey results from 2019 support this. Most teams are too slow deploying new analytics to production—70% take weeks or months. That's really very slow. One big bottleneck is the ability to create analytic development environments. In the same survey, we found that most teams really struggled with this—38% take weeks or months, making it virtually impossible to deliver new analytics quickly.</p><p>This is not surprising because environments in data and analytics are really complex. Creating environments involves a lot of manual steps and delays. For example, you may need to get multiple management approvals, authenticate users who need a sandbox, provision hardware, install and configure software, and replicate data. There's a lot to do. We spoke to one enterprise recently that took 10 to 20 weeks just to complete these tasks.</p><p>To address both development and deployment issues, DataKitchen created kitchens. A kitchen is a sandbox environment where data developers and self service users can do their work. Here you have a typical production pipeline, and say someone wants to make a change in this step here. They can branch off into a test kitchen to work, and then their activity is segregated. Kitchens can be spun up or down on demand, taking what might have been a 10- or 20-week process down to minutes.</p><p>Kitchens really allow for risk free development and empower developers and self service users. This is all possible because the technical environments underlying production and development kitchens match. Development kitchens include everything users need to create new analytics, like a security vault, a Git branch, tools, test data, etc. Then users can safely work in their kitchens without stopping production or each other.</p><p>Testing is also an integral part of Development DataOps. With DataKitchen, automated testing is built into the release and deployment workflows, so testing proves that analytics code is ready to be promoted to production. Instead of a scenario, which often happens today, where analytics are thrown over the wall to be released to production, everyone now can be confident that the new analytics are ready to be released and nothing will break.</p><p>Kitchens then automate the manual steps related to deployment. So when all tests pass and because development and production environments are aligned, a data pipeline can be migrated from a development kitchen to a production kitchen with just a few clicks. This eliminates endless meetings and a lot of the manual processes that really sap a data team’s time.</p><p>The use of kitchens in Development DataOps really greatly enhances collaboration. As I just described, a really common use case is just moving analytics from development to production. So even with a small development team, typically moving the analytics is a multi-step, multi-person, multi-environment process. But kitchens are hierarchical, so child kitchens can be created off a parent kitchen. Developers can then do their work safely in their own environment. And when they're ready, they can merge their collective work up the hierarchy to higher environments into production, knowing that nothing will break. So not only does this allow team members to collaborate easily, all these steps are automated, it saves a ton of time as well.</p><p>When Development DataOps is expanded more broadly to different teams, teams working in different locations, or using different tool chains can now collaborate with ease as well. Each team can build their own pipeline and their respective environments. In this case, we have a team in New Jersey, working in an on-prem environment. They're coordinating with a team in California working in the Cloud. Each team can build their own pipelines using their respective tool chains, and they incorporate tests. These pipelines can then be meta-orchestrated into one workflow by a centralized pipeline that calls the local pipelines. This enables seamless collaboration across the different locations, environments, tools, and teams.</p><p>So the results of Development DataOps can really be tremendous. In this example, going back to Bristol Myers Squibb, you can see that the number of schema changes per week went from 1 to 12. Cycle time to publish new visualizations improved from weeks and months to the next day. And productivity dramatically improved, as the number of data analysts supported by one data engineer increased from 0.5 to 12. So tremendous improvements all around.</p><p>And all of this also leads to dramatic improvements and innovation right? When your teams are more productive, they can be more innovative and responsive to customer requests. As expressed here, “executives want answers as quickly as possible. By using DataKitchen, we were able to mix and match data in new ways, so that we can quickly offer answers to a question.”</p><p>So moving on to the next phase is Measurement DataOps.</p><p>Once you've made progress in phases one and two, it's time to start measuring and improving your processes. We often find it surprising how many analytic teams are un-analytic about their data processes. It's really not the fault of the data teams because it's hard to collect this data. DataKitchen gives you the tools you need to measure and improve what you're doing.</p><p>So in this phase, the platform automates the collection of system-wide process analytics into one combined data store for the analytic system as a whole. This enables you to track production, team and project metrics, as well as process lineage. You're moving up the Lean DataOps hierarchy here. This phase involves multiple teams and can be achieved with some small process changes in some process data integration.</p><p>First, you want to track how your manufacturing facility is doing and get some real time insight into your operations so that you can remove bottlenecks quickly. This report here is an example of our tornado report, which shows production issues and how long they take to resolve. Other types of reports are available that show you things like the status of builds—are data sources on time and are builds on track?—so you can resolve these issues before they reach the customer.</p><p>Measurement DataOps also allows you to measure and improve project and team performance. Here data gives you a bird's eye view, and you can tell whether projects are being completed on time, or whether build times are improving or not. These analytics are also critical for building successful teams. You can track whether teams are increasing collaboration and improving productivity, speeding deployment, meeting deadlines. This really becomes a great management tool and a way for you to improve your team's performance.</p><p>Measurement DataOps will also help you track process lineage. Many companies track their data lineage but they don't really know anything about all the processes that act upon their data, like the code, test history, and timing steps. Access to this information will help you review execution, troubleshoot, and support those all-important governance activities.</p><p>Finally, once you've done Measurement DataOps, all of this data is really going to help you prove DataOps’ value to your boss. By reviewing and sharing these metrics regularly with your team and key stakeholders, you'll of course be able to continuously improve. But then you also want to showcase these improvements, and it will set you up to advance the cause of DataOps on a larger scale across the enterprise.</p><p>Lastly, we're at Enterprise DataOps. When you're ready, you can expand DataOps and everything you learned in phases one, two, and three, across your organization or business unit. Here's where you'll achieve lasting organizational change. This step involves multiple groups and significant process changes, all supported by DataKitchen. Here, you'll also realize the full benefits of DataOps as it relates to collaboration. Teams across the organization, no matter where they're located or which tools they’re using, will be able to work together seamlessly, which brings huge benefits. We went over an example earlier of teams in multiple locations with a mix of Cloud and on-prem environments working together in the Development DataOps section. You can imagine those results on a much larger scale with teams collaborating across the organization.</p><p>There's a lot more to think about in this phase. Some key considerations are: What's your overall corporate DataOps strategy? What cultural roadblocks may still exist? Another big consideration is how to organize your team. How do you organize your team to be Agile? Do you need to hire DataOps engineers? Do you need a center of excellence, Dojo, or technical service organization? There are many different models that can be successful. At the end of the day, how do you get your team to spend the right ratio of time on development and operations? Most teams today without DataOps, spend less than 3% of their time focused on operations. But those that do DataOps are getting closer to 15%, which is a move in the right direction for sure. In the software world, that ratio is even higher—around 23%. So a key part of Enterprise DataOps is to keep moving your teams in this direction.</p></div></div>
